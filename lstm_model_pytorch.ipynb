{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9dc76c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab as build_vocab\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "51a59b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Course</th>\n",
       "      <th>ID</th>\n",
       "      <th>Department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Principles of Nutrition</td>\n",
       "      <td>BIOL</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nutrition for Fitness and Physical Activity</td>\n",
       "      <td>BIOL</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction to Human Physiology</td>\n",
       "      <td>BIOL</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biotechnology Management</td>\n",
       "      <td>BIOL</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Living Biology at Brown and Beyond</td>\n",
       "      <td>BIOL</td>\n",
       "      <td>Biology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Course    ID Department\n",
       "0                      Principles of Nutrition  BIOL    Biology\n",
       "1  Nutrition for Fitness and Physical Activity  BIOL    Biology\n",
       "2             Introduction to Human Physiology  BIOL    Biology\n",
       "3                     Biotechnology Management  BIOL    Biology\n",
       "4           Living Biology at Brown and Beyond  BIOL    Biology"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"courses.csv\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3e40a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_text = df['Department']\n",
    "text = df['Course']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c00af963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4489\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "tokens = [tokenizer(t) for t in text]\n",
    "counter = Counter(word for seq in tokens for word in seq)\n",
    "vocab = build_vocab(counter, specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "print(\"Vocabulary size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b04bf601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6690, 18])\n"
     ]
    }
   ],
   "source": [
    "sequences = [torch.tensor([vocab[token] for token in seq]) for seq in tokens]\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=vocab[\"<unk>\"])\n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cc754214",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels_text)\n",
    "output_dim = len(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d238a5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels_encoded, test_size=0.2, random_state=17)\n",
    "train_data = TensorDataset(X_train, torch.tensor(y_train))\n",
    "test_data = TensorDataset(X_test, torch.tensor(y_test))\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba225f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1899202",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for i, word in vocab.get_stoi().items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "torch_embeddings = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=len(vocab), embedding_dim=embedding_dim, hidden_dim=64, output_dim=output_dim, num_layers=1):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch_embeddings, freeze=False)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(2*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "47d6785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMClassifier(\n",
      "  (embedding): Embedding(4489, 100)\n",
      "  (lstm): LSTM(100, 64, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=77, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(num_layers=1)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dcafaced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer):\n",
    "    num_batches = len(train_loader)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, targets = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        print(f\"Batch {i+1}/{num_batches}, Loss: {loss.item():.4f}\", end='\\r', flush=True)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, targets = batch\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2bb05078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "Test Accuracy: 8.45% 3.4733\n",
      "Epoch 2/25\n",
      "Test Accuracy: 8.45% 3.9643\n",
      "Epoch 3/25\n",
      "Test Accuracy: 8.45% 3.4571\n",
      "Epoch 4/25\n",
      "Test Accuracy: 8.67% 4.1263\n",
      "Epoch 5/25\n",
      "Test Accuracy: 9.19% 3.1030\n",
      "Epoch 6/25\n",
      "Test Accuracy: 9.87% 2.9270\n",
      "Epoch 7/25\n",
      "Test Accuracy: 10.54%3.9722\n",
      "Epoch 8/25\n",
      "Test Accuracy: 11.66%3.0072\n",
      "Epoch 9/25\n",
      "Test Accuracy: 12.11%2.9849\n",
      "Epoch 10/25\n",
      "Test Accuracy: 12.33%2.3546\n",
      "Epoch 11/25\n",
      "Test Accuracy: 13.68%1.9650\n",
      "Epoch 12/25\n",
      "Test Accuracy: 15.77%3.4657\n",
      "Epoch 13/25\n",
      "Test Accuracy: 15.02%2.7489\n",
      "Epoch 14/25\n",
      "Test Accuracy: 17.71%2.7792\n",
      "Epoch 15/25\n",
      "Test Accuracy: 17.64%2.7333\n",
      "Epoch 16/25\n",
      "Test Accuracy: 20.70%2.1615\n",
      "Epoch 17/25\n",
      "Test Accuracy: 23.32%2.5497\n",
      "Epoch 18/25\n",
      "Test Accuracy: 24.44%1.5184\n",
      "Epoch 19/25\n",
      "Test Accuracy: 25.64%1.8002\n",
      "Epoch 20/25\n",
      "Test Accuracy: 28.62%1.3788\n",
      "Epoch 21/25\n",
      "Test Accuracy: 28.55%0.9237\n",
      "Epoch 22/25\n",
      "Test Accuracy: 31.61%2.3199\n",
      "Epoch 23/25\n",
      "Test Accuracy: 30.57%1.9271\n",
      "Epoch 24/25\n",
      "Test Accuracy: 31.39%0.9272\n",
      "Epoch 25/25\n",
      "Test Accuracy: 32.59%1.1600\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    train(model, train_loader, loss_fn, optimizer)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "921339e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predicted departments for 'Computational Methods in Physics':\n",
      "1. Assyriology: 2.48%\n",
      "2. Russian: 2.42%\n",
      "3. Egyptology: 2.36%\n",
      "4. Chinese: 2.19%\n",
      "5. Slavic Studies: 2.18%\n"
     ]
    }
   ],
   "source": [
    "predict_text = \"Computational Methods in Physics\"\n",
    "tokens = tokenizer(predict_text)\n",
    "sequence = torch.tensor([vocab[token] for token in tokens]).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(sequence)\n",
    "    probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    top_probs, top_indices = torch.topk(probabilities, k=5)\n",
    "    top_probs = top_probs.squeeze().tolist()\n",
    "    top_indices = top_indices.squeeze().tolist()\n",
    "\n",
    "    print(f\"Top 5 predicted departments for '{predict_text}':\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        department = label_encoder.inverse_transform([idx])[0]\n",
    "        print(f\"{i+1}. {department}: {prob*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courseenvfull",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
